---
title: "Lesson3"
date: 2024-11-16
layout: default
---

# 绘制系统

## 技术难度最高的部分

在文字游戏中，虽然不需要复杂的绘制系统，但技术难度依然可能来自于复杂的逻辑和交互设计。对于其他类型的游戏，尤其是图形密集型游戏，技术难度会显著增加。

## 计算机图形学与游戏引擎绘制的区别

### 计算机图形学

计算机图形学关注的是**数学和理论上的准确性**，通常目标是生成某个图形或效果。这个过程的关键要素包括：

- **明确的需求**：需要生成某种图形或视觉效果（例如光照、阴影、模型渲染等）。
- **数学正确性**：实现的效果需要在数学上是正确的，例如通过线性代数计算光照、反射等。
- **硬件无关性**：计算机图形学研究通常假定在理想的硬件条件下进行，着重理论模型的正确性。

简而言之，计算机图形学主要关注的是如何理论上生成和优化图像，可能不太关注实时性能和硬件特性。

### 游戏引擎绘制

游戏引擎的绘制系统不仅要依赖于计算机图形学的理论，还要考虑**实时性和硬件兼容性**。其特点包括：

- **性能要求**：游戏引擎需要考虑在不同设备上实现高效的图形渲染，保证**高帧率**和**低延迟**，并在多样化的硬件上运行流畅。
- **复杂性和兼容性**：除了基础的几何图形绘制外，游戏引擎还需要支持复杂的物体渲染，如毛发、水体、粒子效果等，这些都会增加系统的复杂性。
- **硬件适配**：需要对不同硬件（如 GPU、显示器、渲染管线）有较深的理解和优化。不同的硬件要求可能会影响渲染质量和性能。
- **多任务与资源管理**：游戏引擎不仅需要渲染图形，还涉及多线程、资源管理（如纹理、模型、光照等）、场景管理等。每个细节都需要在性能与画质之间做平衡。

游戏引擎的绘制系统是一个高效、灵活的容器，必须支持在不同的硬件环境中兼容和优化各种效果，保持稳定的**帧率**。

### 挑战

随着**帧率要求和画质要求的提升**，现代游戏引擎面临越来越大的挑战，具体包括：

- **复杂渲染效果的计算**：例如毛发、流体、动态光影等效果的渲染，可能需要占用更多的 GPU 和 CPU 资源。
- **帧率和画质的权衡**：如何在不降低游戏性和设计性的情况下，确保游戏在不同设备上流畅运行，同时保证高质量的图像呈现？
- **硬件适配和优化**：要确保游戏能在各种设备上运行，特别是要兼容不同的图形 API（如 OpenGL、DirectX、Vulkan 等）。

### 性能要求

游戏引擎绘制系统的 CPU 占用率通常要求在**10% - 20%**之间，具体取决于游戏的复杂度、场景的数量、渲染特效等因素。高效的渲染引擎需要对这些资源进行精细管理，以达到稳定的帧率和良好的游戏体验。

## Profiling 测试占用

### 1. 工程的迭代优化

在软件工程中，**迭代优化**是通过不断的调整和改进来提高系统性能和功能的过程。特别是在渲染系统的开发中，迭代优化非常重要。通过多次迭代，可以对程序的不同部分进行优化，从而减少性能瓶颈，提高系统效率。

#### 迭代优化的常见步骤：

- **性能分析**：使用 Profiling 工具，识别程序中占用资源较多的部分。
- **代码优化**：通过修改代码，减少不必要的计算或优化算法。
- **硬件利用**：根据硬件架构的特点，调整渲染流程，提升性能。
- **测试与反馈**：通过反复的测试，验证优化效果，并进行持续改进。

### 2. 软件工程系统中的渲染优化

在现代计算机图形学中，渲染性能直接影响到最终用户的体验。渲染系统需要在有限的资源下生成高质量的图像，因此，渲染过程中的每一环节都需要被仔细优化。

#### 渲染系统的组成部分：

- **渲染管线**：包括从顶点数据、纹理、光源、着色器等多个阶段的处理。
- **GPU 加速**：利用图形处理单元（GPU）来加速渲染过程，降低 CPU 负担。
- **资源管理**：有效管理渲染所需的各种资源（如纹理、模型数据等），避免重复加载和不必要的内存消耗。

#### 渲染性能优化的方法：

- **减少 GPU 负担**：通过合并渲染操作、减少每帧计算量等方式，减少 GPU 的渲染负担。
- **减少内存占用**：合理管理纹理和模型数据，避免重复加载和冗余存储。
- **降低渲染复杂度**：在不影响视觉效果的前提下，降低渲染的复杂度，比如简化模型、减少渲染的物体数量。

### 3. 基础硬件与架构

要理解如何优化渲染系统，首先需要了解基础硬件及其架构。硬件架构直接影响软件的性能，特别是在图形渲染方面，GPU 的架构决定了渲染效率和能力。

#### 主要硬件组件：

- **CPU**：中央处理器，负责控制程序流程，处理非图形计算任务。
- **GPU**：图形处理单元，专门用于处理并行计算任务，如渲染图像、视频解码等。GPU 的性能对图形渲染影响最大。
- **内存**：存储所有图形和渲染数据，包括纹理、顶点数据、缓冲区等。内存的带宽和大小决定了渲染过程中的数据访问速度。

#### 硬件架构对渲染的影响：

- **并行计算能力**：GPU 具有大量的计算核心，适合进行并行处理，如渲染大规模的三角形或像素。
- **内存带宽**：图形渲染需要大量的数据传输，内存带宽影响数据从内存到 GPU 的传输速度。
- **硬件加速**：现代 GPU 支持硬件加速的图形操作，如光照、阴影计算等，能大大提高渲染效率。

### 4. 渲染数据体系的管理

渲染数据体系是指在渲染过程中对各种数据（如顶点、纹理、着色器程序等）的管理和优化。渲染数据的管理对于提升渲染性能至关重要。

#### 渲染数据的类型：

- **顶点数据**：描述 3D 物体的基本几何信息，包括位置、法线、纹理坐标等。
- **纹理**：用来为 3D 物体表面提供细节信息，通常是 2D 图像。
- **着色器程序**：控制渲染效果的程序，决定光照、材质、纹理等的计算方式。

#### 渲染数据的优化：

- **数据结构优化**：使用高效的数据结构（如顶点缓冲区、索引缓冲区等）来存储和传递数据，减少数据冗余。
- **批处理渲染**：通过将多个渲染操作合并成一个批次，减少 CPU 和 GPU 之间的通信开销。
- **纹理管理**：采用如 MIP 映射、纹理压缩等技术来减少内存占用和提高渲染效率。

### Rendering 由哪些要素构成

1. 顶点数据 → 三角面数据 → 光栅化成小像素点 → 根据材质和纹理渲染成颜色。
2. 投影 → 光栅化 → 着色。
   - 没有低频滤波，有纹理的物体由近及远会发生画面抖动，走样。

## SIMD 与 SIMT

### 什么是 SIMD（单指令多数据）？

SIMD（Single Instruction, Multiple Data）是一种并行计算技术，它允许一条指令同时处理多个数据元素。这种方式通常用于需要对大量相同操作的数据进行并行计算的场景。SIMD 通过将多个数据并行传输到处理器的不同处理单元上，显著提高了计算效率。

在 C++ 中，SIMD 通过像 SSE（Streaming SIMD Extensions）这样的硬件扩展来实现。SSE 是 Intel 提供的一套指令集，旨在加速对向量数据的处理。它允许程序在一条指令下同时对多个数据元素进行操作，如对多个浮点数进行加法或乘法等。

#### SIMD 的工作原理

- **数据并行性**：SIMD 适合执行大规模的相同计算任务，如图像处理、科学计算等。举个例子，假设我们需要对一组浮点数进行加法运算，在传统的逐个处理模式下，处理器将一个个元素取出，逐一进行计算；而在 SIMD 模式下，处理器可以在一条指令下同时对多个浮点数进行加法操作。

- **硬件支持**：现代处理器（如 Intel 的 CPU 或 AMD 的 CPU）通常内建 SIMD 指令集（例如 SSE、AVX），这些指令集能够一次性操作多个数据。

#### 示例

如果你在 C++ 中使用 SSE 扩展来处理数据，它看起来可能像这样：

```cpp
#include <xmmintrin.h>  // 包含 SSE 指令集

void addVectors(float* A, float* B, float* C, int n) {
    for (int i = 0; i < n; i += 4) {  // 每次处理4个数据
        __m128 a = _mm_loadu_ps(&A[i]);  // 加载4个浮点数
        __m128 b = _mm_loadu_ps(&B[i]);
        __m128 c = _mm_add_ps(a, b);      // 执行SIMD加法操作
        _mm_storeu_ps(&C[i], c);          // 存储结果
    }
}
```

### 什么是 SIMD（单指令多数据）？

SIMT（Single Instruction, Multiple Threads）是一种并行计算模型，广泛应用于 GPU（图形处理单元）等处理器中。在 SIMT 模型中，多个执行单元（线程）执行相同的指令，但每个线程处理不同的数据。这使得 SIMT 模型特别适合进行数据并行计算，尤其是在处理大量的并行任务时。

SIMT 与 SIMD 有相似之处，但其本质的区别在于计算的粒度和执行的方式。SIMT 强调的是在多个线程上并行执行相同的指令，而 SIMD 则是在多个数据单元上并行执行相同的操作。

#### SIMT 的工作原理

- **线程并行性**：SIMT 模型中的每个线程都可以看作一个独立的计算单元，它们执行相同的指令，但每个线程使用不同的数据。GPU 通常由成千上万的线程组成，这些线程同时在多个处理单元上运行，从而显著提高了计算效率。

- **适用场景**：SIMT 主要用于需要大规模并行计算的场景，比如图像渲染、科学计算、深度学习等任务。比如，现代 GPU 可以在一个时钟周期内启动成千上万个线程来执行某个计算任务。

#### 示例

假设你在使用 CUDA 进行 GPU 编程时，编写了一个简单的加法程序：

```cpp
__global__ void addVectors(float* A, float* B, float* C, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        C[idx] = A[idx] + B[idx];  // 每个线程执行相同的加法操作
    }
}

int main() {
    // 假设已经分配并初始化 A, B 和 C 的数据
    addVectors<<<numBlocks, threadsPerBlock>>>(A, B, C, n);
}
```

### SIMD 与 SIMT 的区别

- **粒度差异**: SIMD 通常处理多个数据元素（如向量、矩阵等），而 SIMT 处理多个线程。SIMD 主要聚焦于数据并行性，SIMT 则主要聚焦于线程并行性。

- **执行方式**:SIMD 在硬件层面上通过单指令来操作多个数据元素，而 SIMT 则是通过单指令控制多个线程并行执行。SIMT 是 GPU 计算的核心模式，而 SIMD 则更常见于 CPU 上。

- **适用场景**:SIMD 更适合于单个计算单元内的数据并行任务，像是 CPU 中的矢量运算；而 SIMT 更适合于大规模的线程并行任务，尤其在 GPU 编程和深度学习中，SIMT 的优势更加突出。

## 显卡与 CPU 的差异

现代显卡（GPU）和中央处理器（CPU）在计算任务中的表现差异明显，特别是在浮点运算能力、架构设计和数据处理方式上。下面我们详细分析这两者的差异。

### 1. 浮点运算能力（FLOPS）

- **显卡（GPU）**：现代显卡的浮点运算能力通常超过 10 teraflops（万亿次浮点运算每秒）。相比之下，显卡拥有更强大的并行计算能力，尤其在需要大量并行处理的任务（如图形渲染、深度学习训练等）中，显卡的表现远超 CPU。
- **中央处理器（CPU）**：CPU 的浮点运算能力通常较低，虽然现代 CPU 具备较强的单线程性能，但它们的核心数量较少，无法像显卡那样在成千上万的并行任务中提供高效的处理。

### 2. 显卡的架构：**费米架构**（Fermi Architecture）

- **GPC（图形处理集群）**：GPC 是显卡中用于执行并行任务的核心计算单元，它由多个 SM（流处理单元）组成，负责处理图像渲染、计算任务等工作。每个 GPC 都可以独立处理特定的计算任务，实现高效的并行处理。
- **SM（流处理单元）**：SM 是显卡中的基本计算单元，负责执行具体的指令操作（如 CUDA 计算）。每个 SM 中有多个核心，这些核心可以并行执行同一指令，但处理不同的数据。

  SM 之间可以通过高速共享内存进行信息交换，实现更高效的协作计算。例如，多个 SM 可以协同工作，处理不同部分的任务或数据，从而加速计算过程。

### 3. 显卡与 CPU 的数据计算方式

- **并行性**：显卡特别适合处理大规模的并行计算任务，像图像渲染、科学计算、机器学习等。其架构通过大量的计算单元（SM 和 GPC）实现了高度的并行性，能够在同一时刻同时处理成千上万的数据元素。
- **CPU 的设计**：与显卡不同，CPU 设计上强调单线程的高效性，虽然现代 CPU 已经支持多核处理，但每个核心的计算能力远强于显卡中的处理单元。CPU 适合执行复杂的控制任务，和要求低延迟的操作，如操作系统的管理、串行计算和单任务处理等。

- **数据计算方式的差异**：显卡的计算方式更倾向于**数据并行性**，它可以同时处理多个数据元素，适合执行大规模相同或相似计算的任务。而 CPU 则强调**任务并行性**，它更擅长于处理需要复杂决策、条件分支或低延迟的任务。

### 4. 总结：GPU 和 CPU 的各自优势

- **显卡（GPU）**：适合进行大规模并行计算任务，如图形渲染、深度学习训练等。其结构优化了大量并行任务的处理能力，能够高效执行浮点运算密集型的任务。
- **中央处理器（CPU）**：适合处理复杂的计算逻辑、单线程密集的任务，以及需要高灵活性和低延迟的应用。尽管 CPU 的浮点运算能力较低，但它在处理决策、控制、输入输出等任务时具有不可替代的优势。

## 数据流动成本与内存访问

在现代计算架构中，数据的流动成本和内存访问的效率对于系统的整体性能起着至关重要的作用。特别是在涉及 CPU 和 GPU 之间的数据传输时，这些因素可能导致显著的性能瓶颈，影响应用的响应速度和稳定性。

### 1. 冯诺依曼架构

冯诺依曼架构是大多数计算机系统的基础架构。它的核心特点是将计算单元（CPU）与存储单元（内存）分开。数据和指令通过总线在 CPU 和内存之间传递。尽管冯诺依曼架构已经存在多年，但其局限性也逐渐显现，尤其在涉及高速数据访问时。

#### 1.1 数据流动的成本

冯诺依曼架构中，数据流动存在显著的成本，主要体现在以下几个方面：

- **数据传输速度**：在 CPU 与 GPU 之间，数据的传输速度相对较慢。尤其是在大型计算任务中，频繁的数据交换会增加延迟，影响任务的执行效率。
- **内存带宽**：CPU 和 GPU 各自有独立的内存系统（如 CPU 使用主内存，GPU 使用显存），而内存带宽的限制使得跨设备的数据传输更加缓慢。

### 2. CPU 到 GPU 的数据传递

在一些高性能计算任务，尤其是图形渲染和深度学习训练中，CPU 和 GPU 必须频繁进行数据交换。然而，CPU 到 GPU 的数据传递往往成为性能瓶颈。这种传递有几个挑战：

- **延迟**：由于数据传输的速度较慢，CPU 与 GPU 之间的同步可能导致显著的延迟，特别是在需要快速反馈的实时应用中。
- **不同内存空间**：GPU 的显存和 CPU 的主内存通常是分开的，因此数据在两者之间传递时需要经过显式的复制和同步过程，增加了开销。

如果 CPU 和 GPU 之间的数据传递不够高效，可能会导致画面不同步，甚至影响游戏和应用的流畅度。

### 3. 数据同步问题

在一些要求高帧率和实时反馈的应用场景（例如游戏或虚拟现实）中，CPU 和 GPU 必须保持高度的同步。否则，可能会出现以下问题：

- **画面不同步**：如果数据同步不及时，CPU 计算和 GPU 渲染的内容可能会出现差异，导致画面撕裂或丢帧。
- **较大延迟**：数据传递和同步的延迟可能导致用户体验的下降，尤其是在需要快速响应的互动应用中。

### 4. 游戏引擎中的优化策略

为了减少数据传输带来的性能损耗，现代游戏引擎通常会采取一些优化措施：

- **单向数据传输**：尽量实现 CPU 到 GPU 的单向数据流动，减少不必要的双向传输。这样可以减少 CPU 和 GPU 之间的同步开销。
- **异步计算与渲染**：通过异步计算和渲染技术，游戏引擎可以更好地分配 CPU 和 GPU 的负载，减少等待时间，提高性能。
- **内存管理优化**：通过优化内存管理和数据传输策略，减少不必要的数据复制和同步操作，从而提高整体性能。

## 缓存与计算性能

缓存（cache）在现代计算架构中对性能的影响不可忽视。在数据计算过程中，当 CPU 需要使用某些数据时，首先会检查缓存中是否有这些数据。如果缓存命中（cache hit），CPU 可以快速访问数据，显著提升效率；但如果缓存未命中（cache miss），CPU 需要从内存中查找数据，这会导致显著的性能下降，增加延迟。

### 1. 缓存未命中的影响

缓存未命中通常会带来以下几方面的影响：

- **延迟增加**：数据从内存加载到缓存的过程需要时间，特别是在处理大规模数据时，延迟更为明显。
- **性能下降**：频繁的缓存未命中可能导致处理器的计算速度远低于预期，降低整体系统性能。

因此，优化缓存命中率对于提高计算效率至关重要。

## 顶级游戏对 GPU 的使用与优化

现代游戏引擎非常关注如何高效利用 GPU，以最大化图形渲染性能。以下是游戏引擎在使用 GPU 时的一些关键考虑和优化策略。

### 2. 游戏引擎对 GPU 的优化

- **高效的渲染管线**：游戏引擎通常会设计复杂的渲染管线，以确保每一帧的计算和渲染能够高效地分配到 CPU 和 GPU 上，避免不必要的数据传输和同步。
- **减少渲染瓶颈**：通过技术如**批处理（batching）**和**延迟渲染（deferred rendering）**，可以减少 GPU 在处理图形渲染时的瓶颈，提升渲染效率。

### 3. 主机 UMA 架构与引擎架构的区别

在一些主机系统中（如 PlayStation 或 Xbox），采用统一内存架构（UMA）来实现 CPU 和 GPU 共享同一内存空间。与此不同，PC 和许多高端游戏主机使用独立的内存架构，CPU 和 GPU 拥有各自独立的内存系统。这两种架构对游戏引擎的设计有不同的影响：

- **UMA 架构**：由于 CPU 和 GPU 可以访问同一块内存，数据传输更加高效，但可能会导致内存带宽的瓶颈。
- **独立内存架构**：虽然 CPU 和 GPU 内存独立，避免了共享带来的带宽争用，但需要通过高速总线进行频繁的数据传输。

### 4. 移动端 GPU 优化：功耗与性能平衡

移动端设备（如智能手机、平板电脑）在处理 GPU 渲染时，除了性能优化外，还需要特别关注功耗管理。为了提高渲染效率并节省电池能量，移动端设备通常使用如下技术：

- **Tile-based rendering（分块渲染）**：这种技术通过将画面划分为多个小区域（tile），逐块进行渲染，以减少不必要的像素计算和内存带宽的消耗。
- **动态调整渲染质量**：移动端 GPU 通过动态调整分辨率、纹理质量和图形细节等方式，平衡渲染质量与功耗，确保在不同硬件和电池状态下的最佳表现。

## 绘制系统概念

在计算机图形学中，逻辑上可以表达的游戏对象和真实可以绘制的物体是两个不同的概念。我们通常将这些对象分成“逻辑对象”和“渲染对象”。

### 1. 逻辑对象 vs 渲染对象

- **逻辑对象**：在游戏的逻辑中存在的实体，如角色、道具、建筑等。这些对象包含有关行为、状态等的多种信息。
- **渲染对象**：指的是游戏引擎用来在屏幕上绘制物体的具体数据。这些通常包括物体的外观数据，如网格（Mesh）、材质（Material）、纹理（Texture）等。

这些概念之间存在差异，但渲染对象通常是通过对逻辑对象的渲染转换来生成的。

### 2. Mesh Component：可绘制的实体

- **Mesh Component**：指的是可绘制的物体部分，通常包括顶点数据、法线数据、纹理和材质等。在游戏中，角色、环境物体等都是由多个 Mesh 组成的。

例如，一个人物模型通常包括多个 Mesh，如：

- **头盔 Mesh**
- **衣服 Mesh**
- **胡须 Mesh**

这些 Mesh 组件具有不同的材质和纹理，例如，头盔可能使用金属材质，衣服使用布料材质，胡须则可能是毛发纹理。

## Mesh 和材质

在 3D 图形渲染中，**Mesh** 和 **材质** 是两大重要概念，它们共同决定了物体的外观。

### 1. Mesh 结构

- **Mesh Primitive（网格原始体）**：在图形学中，Mesh 由一系列的顶点（Vertex）构成，这些顶点定义了物体的几何形状。每个顶点包含以下数据：
  - **位置（Position）**：顶点在三维空间中的坐标。
  - **颜色（Color）**：顶点的颜色信息。
  - **法线（Normal）**：法线方向定义了顶点表面的朝向，影响光照计算。
- **三角面（Triangle）**：一个 Mesh 由若干三角形组成，每个三角形由三个顶点定义。为了节省内存，通常我们不会重复存储顶点的具体数据，而是通过**索引值**来引用这些顶点。这意味着，多个三角形可以共享相同的顶点数据。

  - 例如，假设有一个正方体，正方体的每个面都由两个三角形组成，但所有的顶点可以被多个三角形共享。通过使用顶点索引，可以避免重复存储相同的顶点数据，从而减少内存的消耗。

### 2. 法线（Normal）方向的重要性

每个顶点通常需要定义法线（Normal）方向，原因如下：

- **折面处理**：当多个三角形的顶点在连接处形成折面时，顶点的法线方向至关重要。不同的法线方向会导致光照计算的结果有所不同，影响最终渲染效果。
- **平滑阴影和光照效果**：在光照计算中，顶点法线方向用于确定光线与表面的角度，从而决定物体表面的光照强度。多个相邻三角形的法线方向将影响光照的平滑度和阴影效果。

例如，当一个物体的表面由多个三角形构成时，法线定义了每个三角形面朝的方向。如果法线朝向不同的方向，渲染时就会出现折面效果；而如果法线朝向一致，渲染出来的效果会显得更平滑。

## 材质和纹理

在 3D 渲染中，**材质**和**纹理**是物体外观的关键部分。

### 1. 材质（Material）

- **材质**定义了物体表面的光照特性。它包括物体表面如何反射光线、如何吸收光线以及物体表面的颜色等信息。
- **常见材质类型**：
  - **漫反射（Diffuse）**：表面在各个方向上均匀地反射光线。
  - **镜面反射（Specular）**：表面反射光线的强度和方向决定了镜面光泽效果。
  - **金属（Metallic）**：定义金属表面的光反射特性，通常具有较强的镜面反射效果。

### 2. 纹理（Texture）

- **纹理**是附着在物体表面的一种图像或图案，通常用来给物体表面添加细节。例如，布料的表面可能会有细微的花纹，而金属表面可能会有划痕或锈迹的纹理。
- 纹理是通过映射技术（Texture Mapping）应用到物体的表面，通常通过 UV 映射将纹理图像与物体的几何形状相对应。

## 材质和物理材质

在 3D 渲染中，**材质**不仅仅是物体表面颜色的简单描述，它还决定了物体表面如何与光相互作用。随着技术的发展，材质模型经历了多个重要的迭代，以下是常见的几种材质模型：

### 1. Phong 模型

**Phong 模型**是一种经典的材质模型，用于模拟表面光照的反射特性。它通过三个主要的反射成分来描述光照效果：

- **环境光（Ambient）**：模拟来自环境的均匀光照，通常不受物体表面角度的影响。
- **漫反射（Diffuse）**：模拟光线与粗糙表面反射的情况，表面上每个点都以相同的强度反射光线。
- **镜面反射（Specular）**：模拟光线与光滑表面的反射，通常会产生亮点（高光），并且该亮点会根据视角发生变化。

尽管 Phong 模型在早期被广泛使用，但它的简化假设和不真实的反射效果导致它逐渐被更复杂的模型替代。

### 2. 基于物理的渲染（PBR）

**PBR（基于物理的渲染，Physically Based Rendering）** 是一种更为真实的材质模型，旨在模拟物理世界中的光照和表面交互。与传统的材质模型不同，PBR 强调以下几个方面：

- **能量守恒**：反射的光线总量不能超过入射光线的总量。
- **金属与非金属材质的区分**：PBR 区分了金属表面和非金属表面的反射特性。
- **微表面模型**：PBR 使用微表面模型来模拟粗糙表面如何影响光的反射，通常使用**法线贴图（Normal Map）**和**粗糙度贴图（Roughness Map）**来控制这一效果。

PBR 提供了更加真实和一致的渲染效果，特别是在不同光照环境下，物体表面表现的方式更加自然。

### 3. Subsurface Scattering（SSS，次表面散射材质）

**次表面散射（Subsurface Scattering，SSS）**是一种用于模拟半透明材质的效果，例如皮肤、蜡、牛奶等。SSS 模型能够模拟光线进入表面并在内部散射，最终从表面重新射出。这种效果通常用于表现物体的**厚度**和**透光性**。

- **皮肤**是一个常见的例子，光线进入皮肤后会散射并在皮肤下层发生反射，最终带有柔和的光感效果。

SSS 是 PBR 中的一个重要补充，可以提高材质的真实性，尤其是在需要表现光线通过物体的场景中。

### 4. 纹理的重要性

在材质的定义中，**纹理**扮演着至关重要的角色。纹理不仅仅是简单的装饰，它在 PBR 等现代渲染模型中决定了物体表面反射的真实效果。材质的参数（如粗糙度、金属度）虽然在某些情况下影响光照反射，但纹理的细节（例如法线纹理、粗糙度纹理）才是物体表面视觉表现的决定性因素。

例如，在 PBR 中：

- **金属度贴图（Metallic Map）**决定了表面是否为金属。
- **粗糙度贴图（Roughness Map）**控制表面反射的平滑程度。
- **法线贴图（Normal Map）**影响表面细节的表现，模拟细小的凸起和凹陷。

因此，理解和合理使用纹理是提高渲染效果的关键。

## Shader 的作用

在现代图形渲染中，**Shader** 是实现各种视觉效果的核心工具。**Shader** 是一种小程序，运行在 GPU 上，负责计算像素或顶点的最终显示效果。

### 1. Shader 的基本功能

**Shader** 的作用是将场景中的数据（如顶点、纹理、光照信息等）转化为最终在屏幕上显示的图像。具体来说，Shader 会操作以下数据：

- **顶点数据**：包括物体的几何形状、位置、颜色等信息。
- **纹理数据**：如表面细节、法线、粗糙度、金属度等。
- **光照数据**：光源的类型、强度、方向等信息。

### 2. 渲染过程中的 Shader

在渲染过程中，Shader 执行的步骤可以大致分为以下几个阶段：

- **顶点着色器（Vertex Shader）**：处理物体的每个顶点，进行坐标变换、法线计算等。
- **片段着色器（Fragment Shader）**：负责计算最终像素的颜色和光照效果，通常会结合材质、纹理和光照信息。
- **几何着色器（Geometry Shader）**：处理图形的几何形状，可以在此阶段生成新的几何体（如点、线、三角形等）。

Shader 代码操作的对象通常是**可渲染的数据（Renderable Data）**，这些数据在渲染过程中不断被传递和计算，最终绘制成图像。

### 3. Shader 的种类

Shader 的种类非常多，不同的渲染阶段会使用不同类型的 Shader，常见的包括：

- **顶点着色器（Vertex Shader）**
- **片段着色器（Fragment Shader）**
- **几何着色器（Geometry Shader）**
- **计算着色器（Compute Shader）**：用于执行通用计算任务，广泛应用于物理模拟、粒子系统等。

通过使用不同的 Shader，开发者可以实现丰富的图形效果，如光影、反射、折射等。

## GPU 批处理渲染（Batch Rendering）

批处理渲染是一种优化渲染性能的技术，通过将物体按照相同材质进行排序和合并，减少渲染过程中的状态切换，从而提高效率。

- **合批渲染**：将使用相同材质的物体集中在一起进行渲染，避免每个物体都单独调用渲染。这样能显著减少 GPU 处理中的材质切换开销。
- **更换材质时的优化**：当渲染过程中需要切换材质时，通过批量处理相似材质的物体，避免频繁切换，进一步提升渲染效率。

这种技术在渲染复杂场景时尤为重要，能显著减少 GPU 的负载。

---

## 可见性裁剪（Culling）

可见性裁剪是指在渲染前判断哪些物体是不可见的，从而避免不必要的渲染操作，提高性能。通常，裁剪是通过判断物体是否在视线范围内来实现的。

### 视线锥（Frustum Culling）

- **视线锥**（Frustum）是一个定义视图范围的几何体，通常呈金字塔形或矩形锥形。只有在视线锥内的物体才需要被渲染，视线锥外的物体会被剔除，从而节省计算资源。
- **裁剪**：通过检查物体的包围盒（Bounding Box）是否在视线锥内，来确定该物体是否需要渲染。这样可以显著提高渲染效率，避免渲染那些用户无法看到的物体。

在实际应用中，这一过程通常在渲染前进行，以确保只绘制那些最终会出现在屏幕上的物体。

---

## 包围盒（Bounding Volume）

为了进行有效的可见性裁剪，通常需要给每个物体定义一个**包围盒**。包围盒是一种简化的几何体，用于围绕物体的形状，以便快速判断物体是否可见。

常见的包围盒类型包括：

1. **包围球（Bounding Sphere）**

   - **特点**：最简单和最紧凑的包围盒形状，计算效率高，适用于快速裁剪。
   - **优点**：球形计算较简单，尤其适合旋转物体。
   - **缺点**：有时包围盒会比物体本身大很多，可能会导致裁剪不够精确。

2. **轴对齐包围盒（AABB）**

   - **特点**：一个与坐标轴对齐的矩形盒子。
   - **优点**：计算非常简单，适用于静态物体。
   - **缺点**：当物体旋转时，包围盒可能不再紧密，导致效率下降。

3. **方向包围盒（OBB）**

   - **特点**：包围盒与物体的方向对齐，能更精确地围绕物体。
   - **优点**：适用于旋转的物体，裁剪更精确。
   - **缺点**：计算开销较大。

4. **凸包（Convex Hull）**
   - **特点**：通过计算物体的外部形状，生成最紧凑的包围盒。
   - **优点**：精度最高，适合动态物体。
   - **缺点**：计算量较大，可能影响性能。

---

## 资源池（Resource Pool）

资源池（Resource Pool）是一种将不同的渲染资源（如材质、纹理、顶点数据等）进行统一管理的技术，旨在优化性能并减少重复的资源加载。

### 资源池的概念

在图形渲染中，一个物体通常由多个元素组成，比如顶点数据、材质、纹理等。如果我们将每个物体的资源（如材质和纹理）分开存储和管理，可能会导致大量的重复数据。这种做法既浪费内存，又增加了渲染的复杂度。资源池的核心目的是将这些资源统一管理，避免重复加载，从而提升性能。

#### 材质和纹理的管理

- 一个物体的材质和纹理不应为每个物体独立设置，而应该对所有物体共享相同的材质和纹理，减少内存消耗。
- **材质不同的部分可以切分为 submesh**。例如，一个复杂物体可能由多个材质组成，但它们可以共享相同的纹理和顶点数据。此时，`submesh` 就能将每种材质分离出来，每个子网格（submesh）只需要存储它自己的偏移量（offset），而所有的顶点三角形数据可以存储在一个大的缓冲区中。

### 如何实现资源池

资源池通过以下方式组织数据：

1. **顶点数据和三角形数据**：所有物体的顶点数据（如位置、法线、纹理坐标等）可以统一存储在一个大的顶点缓冲区中，避免每个物体都拥有单独的缓冲区。
2. **submesh 结构**：每个 `submesh` 存储该部分材质的偏移量和大小信息，而不重复存储顶点数据。`submesh` 只是对原始缓冲区中数据的一个引用，它指向顶点缓冲区的特定部分。
3. **减少材质切换**：由于 `submesh` 结构可以将相同材质的部分聚合在一起，这样渲染时可以减少材质切换，从而提高渲染效率。

#### 例子

假设我们有一个物体，它由两个部分组成：一个是有纹理的金属部分，另一个是有玻璃材质的部分。我们可以将两个部分分别切分为两个 `submesh`，而这两个 `submesh` 可以共享同一个顶点数据缓冲区。这样，我们只需要为每个 `submesh` 存储材质的偏移量和大小，而不需要为每个物体重复存储顶点数据。

- **顶点缓冲区**：存储物体的所有顶点数据。
- **材质缓冲区**：存储不同材质的信息。
- **submesh 信息**：每个 `submesh` 只需要存储该部分材质的偏移量，指向顶点缓冲区中的一部分数据。

## 空间划分优化与纹理压缩

### 空间划分优化

空间划分是一种优化技术，用于减少计算量、提高渲染效率。通过将大场景划分成多个小区域，可以更有效地进行渲染和碰撞检测。常见的空间划分方法有以下几种：

#### 1. 四叉树（Quadtree）

四叉树是一种递归的空间划分方法，适用于二维空间。通过不断将空间划分为四个子区域（每个区域一个子树），将对象根据其位置分配到不同的节点，从而加速物体之间的交互检测和渲染。

- **应用场景**：四叉树常用于处理 2D 场景中的物体分布和碰撞检测，特别适用于需要大量空间搜索的场景。
- **优点**：能够有效地减少不必要的计算，避免对所有物体进行遍历。

#### 2. BVH（Bounding Volume Hierarchy）

BVH（包围盒层次结构）是一种常用于 3D 图形中的空间划分方法。通过将物体包装在一系列的包围盒内，并对这些包围盒进行层次划分，从而减少与其他物体的碰撞检测和光线追踪计算量。

- **应用场景**：广泛应用于光线追踪、物体碰撞检测等需要处理 3D 空间的场景。
- **优点**：高效地减少了物体间的计算量，尤其适用于复杂的三维场景。

#### 3. PVS（Potentially Visible Set）

PVS（潜在可见集）是一种基于空间划分的优化技术，常用于提高场景的加载和渲染效率。通过将空间划分为若干小房间，并记录每个房间可见的其他房间，PVS 可以在渲染时只加载可能被用户看到的区域。

- **工作原理**：将大场景划分为多个小区域，每个区域内记录它可能看到的其他区域。当玩家位于某个区域时，游戏引擎只加载该区域和其潜在可见的区域，避免加载不需要的内容。
- **优点**：减少不必要的资源加载，提高游戏运行时的效率，特别适用于大型开放世界游戏。

#### 4. GPU Culling

GPU Culling（GPU 剔除）是一种利用 GPU 硬件进行遮挡剔除的优化技术。通过先绘制深度信息，GPU 可以在渲染过程中自动判断哪些物体被遮挡并丢弃它们的渲染。

- **工作原理**：GPU 首先计算场景中的深度信息，并根据物体的深度是否被其他物体遮挡来决定是否渲染它们。被遮挡的物体不会被绘制，从而减少了计算和渲染负担。
- **优点**：通过利用 GPU 的并行计算能力，可以显著减少渲染不必要的物体，提升渲染性能。

### 纹理压缩

在游戏开发中，纹理资源通常占用大量内存和带宽，因此纹理压缩技术是提升性能的重要手段。

#### 1. 常规压缩算法 vs 专用纹理压缩

常规压缩算法（如 PNG）对于普通图片格式很有效，但不适合游戏中的纹理压缩。原因在于常规压缩算法通常是按像素压缩的，无法提供对纹理数据的高效随机访问，这对于实时渲染是一个大问题。

- **PNG 等格式**：这些格式是通用的压缩方法，压缩后无法直接按纹理块进行高效的读取和访问，不适合高效图形渲染中的应用。

#### 2. Block Compression（区块压缩）

区块压缩（Block Compression）是一种专为图形渲染优化的纹理压缩技术，通常应用于 GPU 中。在这种方法中，纹理被分为较小的“块”，每个块内的数据会被压缩为更少的位数，从而减少内存占用并提高加载效率。

- **工作原理**：每个压缩块代表一个纹理区域，采用适当的颜色值来表示该区域的亮暗变化。这样可以在加载时快速解压缩和访问该区域的数据，而不需要解压整个纹理。
- **常用格式**：如 DXT（S3TC）、BCn 等格式，是目前在游戏引擎中广泛使用的纹理压缩格式。
- **优点**：大大减少纹理的内存占用，并提高纹理数据的加载速度，尤其适用于需要快速渲染的实时图形应用。

## 渲染工具与流程

渲染工具涉及多种技术和步骤，主要包括模型处理、纹理映射、Shader 编程等。渲染过程的核心数据结构之一是**Renderable**，它代表了在渲染过程中可视化的所有对象。

### 1. Renderable：渲染系统中的核心数据

**Renderable**是渲染系统中至关重要的数据结构，它包含了用于渲染的所有信息，如模型、纹理、着色器等。渲染工具的目标是通过这些数据来生成最终的图像。常见的渲染工具和流程包括：

- **模型处理**：将 3D 模型数据处理为 GPU 可以高效渲染的格式。
- **纹理映射**：为模型表面应用纹理，以增强视觉效果。
- **Shader 编程**：使用着色器程序（如顶点着色器、片段着色器等）来控制渲染的每一个阶段。

---

### 2. 现代 Mesh 处理

随着图形技术的发展，3D 模型的复杂度大大增加，传统的渲染方法已无法高效处理这些庞大的数据。现代渲染系统采用了更加高效的**Mesh 处理**技术，特别是在处理大规模模型和细节时。

#### 2.1. **Poly Modeling**与**程序化生成**

- **Poly Modeling**：指通过手动建模来创建多边形网格。随着模型复杂度的增加，手工建模可能无法满足实时渲染的需求。
- **程序化生成**：使用算法自动生成 3D 模型，尤其适用于需要大量重复性内容的场景（如地形生成、城市建筑等）。

#### 2.2. **Cluster-Based Mesh Pipeline**（基于集群的网格处理管线）

当处理非常精细和复杂的模型时，**Cluster-Based Mesh Pipeline**是一种常用的优化技术。它将模型分割成小的**Cluster**（集群）或**Mesh 片段**，每个集群通常包含 32 或 64 个面片。这种方法有几个关键优势：

- **提高显卡计算效率**：将模型分成较小的片段后，显卡可以在计算时更高效地处理这些小区域，减少冗余计算。
- **数据局部性**：通过聚集相似的面片，提升了数据的局部性，避免了内存访问的瓶颈。

这种处理方法大大提高了渲染效率，特别是在需要细节丰富的模型场景中（如游戏或电影中的角色和环境）。

---

### 3. Shader 的演进

着色器（Shader）是图形渲染管线中用于定义如何处理顶点和像素的程序。从早期的基础着色器到现代的高效高级着色器，Shader 的演进极大地推动了图形技术的发展。

#### 3.1. **早期的 Hull Shader 和 Domain Shader**

- **Hull Shader**：在图形管线中，Hull Shader 主要用于生成曲面细节，通常与细分曲面（Tessellation）技术一起使用。
- **Domain Shader**：Domain Shader 与 Hull Shader 配合工作，负责计算细分后的顶点数据。

这两种 Shader 的出现使得图形能够呈现出更为平滑和细致的曲面，特别适用于需要动态细节的场景。

#### 3.2. **Geometry Shader：生成更多细节**

- **Geometry Shader**：该着色器能在顶点着色器之后生成额外的几何细节。它不仅可以对传入的几何体进行变换，还可以创建新的几何元素，如点、线或三角形。

  - **应用**：常用于粒子系统、线框图形、及其他需要动态生成几何形状的场景。

#### 3.3. **Mesh Shader：基于数据生成多样化的集合**

**Mesh Shader**是现代图形渲染管线中的一种重要技术，它与传统的渲染管线有所不同。Mesh Shader 允许开发者基于网格数据的相似性生成大量集合，并在此基础上选择不同的计算精度，从而优化性能。

- **工作原理**：Mesh Shader 通过对数据的“集合”进行处理，可以在一个 Pass 中生成多个子集，并根据渲染需求选择合适的精度，最大程度地提高计算效率。
- **应用场景**：Mesh Shader 特别适用于需要处理大量顶点和复杂几何形状的场景，例如高细节的环境渲染或复杂角色的细节处理。

---

## GPU 驱动渲染（GPU Driven Rendering）

GPU 驱动渲染是一种优化图形渲染性能的技术，旨在将渲染任务的处理转移到 GPU（图形处理单元），以减少 CPU 的负担并提高整体渲染效率。通过将更多的计算任务交给 GPU 执行，能够加速图形渲染过程，特别是在处理复杂场景或大规模数据时，能够显著提升性能。

### 1. 什么是 GPU 驱动渲染？

GPU 驱动渲染（GPU-Driven Rendering）是一种将渲染计算交由 GPU 执行的技术。在传统的渲染管线中，CPU 通常会负责协调和控制渲染流程，而 GPU 则执行具体的图形计算任务。而 GPU 驱动渲染则是进一步将渲染逻辑移交给 GPU，由 GPU 来完成大部分的渲染任务和数据处理。

#### 1.1. 传统渲染与 GPU 驱动渲染的区别

- **传统渲染管线**：在传统的渲染管线中，CPU 通常负责大部分的渲染管理工作，比如数据的准备、命令发起等。GPU 主要负责图形计算和像素渲染。
- **GPU 驱动渲染**：与此不同，GPU 驱动渲染使得 GPU 不仅负责底层的图形计算任务，还能根据指令自主管理和调度渲染任务，从而减少 CPU 的干预和负担。

这种方式能够充分利用 GPU 的并行计算能力，从而提升渲染效率，特别适用于需要处理大量图形数据或复杂场景的情况。

### 2. 为什么要使用 GPU 驱动渲染？

使用 GPU 驱动渲染有许多优势，尤其是在现代图形渲染中，能够有效提高性能和效率。

#### 2.1. 降低 CPU 负担

在传统的渲染方法中，CPU 需要处理大量的渲染调度和资源管理工作，这会导致 CPU 负载过高。将这些任务转交给 GPU 后，CPU 可以专注于其他任务，比如游戏逻辑、物理模拟等，从而提高整个系统的响应速度和效率。

#### 2.2. 提高并行处理能力

GPU 具有大量的并行计算单元，非常适合用于处理复杂的图形计算任务。在传统渲染中，CPU 需要串行处理渲染数据，这样会导致处理效率较低。而 GPU 驱动渲染可以通过高度并行的计算模式，快速处理大量图形数据，从而显著加速渲染过程。

#### 2.3. 优化渲染流程

GPU 驱动渲染还可以帮助优化渲染管线，使得渲染过程更加高效。GPU 可以直接控制渲染命令和数据的传输，避免了 CPU 与 GPU 之间频繁的通信和数据传输，提高了渲染流水线的吞吐量。

### 3. GPU 驱动渲染的工作原理

GPU 驱动渲染通常涉及以下几个步骤：

#### 3.1. 数据准备与传输

首先，渲染所需的数据（如几何体、纹理、着色器等）从 CPU 传输到 GPU。传统的渲染过程中，CPU 会先进行大量的计算和数据处理，而 GPU 驱动渲染则要求这些数据在传输到 GPU 后，能够迅速开始处理，而不依赖于 CPU 的干预。

#### 3.2. 任务调度与处理

一旦数据被传输到 GPU，GPU 就可以通过其强大的并行计算能力执行渲染任务。GPU 驱动渲染通过利用 GPU 的计算单元，将渲染任务分配到多个计算单元上并行执行，从而提高渲染效率。

#### 3.3. 渲染输出与显示

渲染过程中生成的最终图像将从 GPU 输出并显示到屏幕上。与传统渲染相比，GPU 驱动渲染的关键优势在于可以减少 CPU 的操作，使得整个渲染过程更加高效。

### 4. 应用场景与优势

GPU 驱动渲染技术在许多领域具有广泛的应用，尤其是在需要高效处理大量图形数据的场景中。

#### 4.1. 游戏开发

在现代游戏中，场景通常非常复杂，包含大量的模型、光照、粒子效果等，传统的 CPU 渲染方法往往无法满足高帧率和流畅度的需求。GPU 驱动渲染可以充分发挥 GPU 的并行处理能力，提高渲染效率，保证游戏的流畅运行。

#### 4.2. 电影特效制作

电影中的特效渲染通常需要处理非常复杂的图形和大量的动态数据。GPU 驱动渲染能够加速渲染过程，使得电影特效的制作更加高效。

#### 4.3. 虚拟现实与增强现实

虚拟现实（VR）和增强现实（AR）要求极高的实时渲染性能，因为任何延迟都会影响用户的沉浸感。GPU 驱动渲染能够提供更高的渲染性能，确保 VR/AR 场景的流畅体验。

[back](https://fenglimg.github.io/skills-github-pages/2024/11/03/Games104-Notes.html)
